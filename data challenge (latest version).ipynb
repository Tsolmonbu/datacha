{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T14:54:28.476729636Z",
     "start_time": "2024-02-15T14:54:28.469616163Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import curve_fit\n",
    "import time\n",
    "from scipy.stats import gaussian_kde\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import fsolve\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T14:54:34.259526547Z",
     "start_time": "2024-02-15T14:54:32.176227689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time (months)</th>\n",
       "      <th>crack length (arbitary unit)</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.003686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.014691</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.048912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.000898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.007360</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>6</td>\n",
       "      <td>0.048160</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>7</td>\n",
       "      <td>0.119816</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>8</td>\n",
       "      <td>0.075937</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>9</td>\n",
       "      <td>0.017686</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>10</td>\n",
       "      <td>0.053080</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>667 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     time (months)  crack length (arbitary unit)  item_id\n",
       "0                0                      0.003686        0\n",
       "1                1                     -0.014691        0\n",
       "2                2                     -0.048912        0\n",
       "3                3                     -0.000898        0\n",
       "4                4                     -0.007360        0\n",
       "..             ...                           ...      ...\n",
       "662              6                      0.048160       49\n",
       "663              7                      0.119816       49\n",
       "664              8                      0.075937       49\n",
       "665              9                      0.017686       49\n",
       "666             10                      0.053080       49\n",
       "\n",
       "[667 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize an empty DataFrame to hold all the testing degradation data\n",
    "all_test_data = pd.DataFrame()\n",
    "\n",
    "# Path to the directory containing the item files\n",
    "items_directory = './robot_maintenance/testing_data/sent_to_student/group_0'\n",
    "#items_directory = './robot_maintenance/training_data/degradation_data'\n",
    "\n",
    "# Loop through all item files and concatenate data for relevant items\n",
    "for item_id in range(50):\n",
    "    file_path = os.path.join(items_directory, f'testing_item_{item_id}.csv')\n",
    "    if os.path.exists(file_path):  # Check if the file exists\n",
    "        item_data = pd.read_csv(file_path)\n",
    "        # Add an 'item_id' column to the DataFrame\n",
    "        item_data['item_id'] = item_id\n",
    "        # Concatenate this item's data into the aggregate DataFrame\n",
    "        all_test_data = pd.concat([all_test_data, item_data], ignore_index=True)\n",
    "\n",
    "# Now all_test_data contains the aggregated testing data for all items\n",
    "all_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pf_class:\n",
    "    '''\n",
    "    This is the definition of particle filetering class.\n",
    "\n",
    "    Methods:\n",
    "    - __init__: Initialization function.\n",
    "    - state_estimation(): Implement the particle filtering.\n",
    "    - rul_prediction(): Predict RUL.\n",
    "    - resample(): Perform resampling in state estimation.\n",
    "    - get_state_estimation(): This function allows estimating the mean and CI based on samples and weights.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, Ns, t, nx, gen_x0, sys, obs, p_yk_given_xk, gen_sys_noise, degradation_path=[], initial_outlier_quota=3):\n",
    "        '''\n",
    "        Initialization function of the PF class.\n",
    "\n",
    "        Args:\n",
    "        - Ns: Particle size.\n",
    "        - t: An array of the measurement time.\n",
    "        - nx: Number of state variables, a scalar value.\n",
    "        - w: weights   (Ns x T), where T is the total number of measurements.\n",
    "        - particles: particles (nx x Ns x T).\n",
    "        - gen_x0: function handle of a procedure that samples from the initial pdf p_x0 to create initial particles.\n",
    "        - p_yk_given_xk: function handle of the observation likelihood PDF p(y[k] | x[k]).\n",
    "        - gen_sys_noise: function handle of a procedure that generates system noise.\n",
    "        - sys: function handle to process equation.\n",
    "        - obs: function handle to observation equation.\n",
    "        - degradation_path: function handle to calcualte the degradation measures.\n",
    "        - initial_outlier_quota: The number of concecutive outliers allowed, before reinitiating the particles.\n",
    "        '''\n",
    "        self.k = 0 # Current step.\n",
    "        self.Ns = Ns # Particle size.\n",
    "        T = len(t) # Get the number of measuring points.\n",
    "        # Memory assignment.\n",
    "        self.w = np.zeros((Ns, T)) # Weights.\n",
    "        self.t = t # Observation time\n",
    "        self.particles = np.zeros((nx, Ns, T)) # Particles.\n",
    "        # Store the function handles of the state space model.\n",
    "        self.gen_x0 = gen_x0\n",
    "        self.sys = sys\n",
    "        self.obs = obs\n",
    "        self.p_yk_given_xk = p_yk_given_xk\n",
    "        self.gen_sys_noise = gen_sys_noise\n",
    "        self.outlier_quota = initial_outlier_quota\n",
    "        self.initial_outlier_quota = initial_outlier_quota\n",
    "        self.degradation_path = degradation_path\n",
    "\n",
    "\n",
    "    def state_estimation(self, yk, resampling_strategy='multinomial_resampling'):\n",
    "        \"\"\"\n",
    "        This function implement a generic particle filter to estimate the system states. Note: when resampling is performed on each step this algorithm is called\n",
    "        the Bootstrap particle filter.\n",
    "\n",
    "        Usage:\n",
    "        xhk = pf.state_estimation(yk, resamping_strategy)\n",
    "        This function has to be called sequentially, at k=1,2,\\cdots,\n",
    "\n",
    "        Args:\n",
    "        - yk   = observation vector at time k (column vector)\n",
    "        - resampling_strategy = resampling strategy. Set it either to\n",
    "                                'multinomial_resampling' (default) or 'systematic_resampling'\n",
    "\n",
    "        Outputs:\n",
    "        * xhk   = estimated state\n",
    "\n",
    "        Reference: Arulampalam et. al. (2002).  A tutorial on particle filters for online nonlinear/non-gaussian bayesian tracking. IEEE Transactions on Signal Processing. 50 (2). p 174--188\n",
    "\n",
    "        This function was developed based on an original verision in Matlab programmed by: Diego Andres Alvarez Marin (diegotorquemada@gmail.com), February 29, 2012.\n",
    "        \"\"\"\n",
    "        # Get the current step.\n",
    "        t = self.t\n",
    "        k = self.k\n",
    "        if k == 0:\n",
    "            raise ValueError('error: Cannot have only one step!')\n",
    "\n",
    "        # Initialize variables\n",
    "        Ns = self.Ns  # number of particles\n",
    "        # If it is the first data point, we need to create the initial particles.\n",
    "        if k == 1:\n",
    "            self.particles[:, :, 0] = self.gen_x0(Ns)  # Generate the initial particles.\n",
    "            self.w[:, 0] = np.repeat(1 / Ns, Ns)  # All particles have the same weight initially.\n",
    "\n",
    "        # Separate memory\n",
    "        xkm1 = self.particles[:, :, k-1]  # extract particles from last iteration;\n",
    "        wkm1 = self.w[:, k-1]  # weights of last iteration\n",
    "        xk = np.zeros_like(xkm1)  # Initial values for the current particles.\n",
    "        wk = np.zeros_like(wkm1)  # _ for the current weights.\n",
    "\n",
    "        # Algorithm 3 of Ref [1].\n",
    "        uk = self.gen_sys_noise(Ns) # Generate the process noise.\n",
    "        for i in range(Ns): # For each particle, predict its state in the next time instant.\n",
    "            xk[:, i] = self.sys(t[k], xkm1[:, i], uk[:, i]) # This is the state equation.\n",
    "            wk[i] = wkm1[i] * self.p_yk_given_xk(yk, xk[:, i]) # Update the weights (when using the PRIOR pdf): eq 63, Ref 1.\n",
    "        # Handle exception:\n",
    "        if sum(wk) == 0: # If sum(wk)==0: Keep the previous weigths.\n",
    "            if self.outlier_quota == 1:\n",
    "                # print(f'Reinitiate the particles: k={k}')\n",
    "                xk = self.gen_x0(Ns, t[k])\n",
    "                wk = np.repeat(1 / Ns, Ns)\n",
    "                self.outlier_quota = self.initial_outlier_quota\n",
    "            else:\n",
    "                # print(f'Smoothing due to weight NaN: k={k}')\n",
    "                for i in range(Ns):\n",
    "                    xk[:, i] = self.sys(t[k], xkm1[:, i], np.zeros(xkm1.shape[0]-1))\n",
    "                wk = wkm1\n",
    "                self.outlier_quota -= 1\n",
    "        else: # Here we scrape the outlier.\n",
    "            y_tmp = xkm1[-1, :] # This is the degradation estimation of each particles.\n",
    "            y_w = wkm1\n",
    "            y_mean = self.sys(t[k], np.matmul(xkm1, wkm1), np.zeros(xkm1.shape[0]-1))[-1]\n",
    "            _, y_bands = self.get_state_estimation(y_tmp, y_w) # Get the 90% CI of the estimation.\n",
    "            interval_width = y_bands[1]-y_bands[0]\n",
    "            # A outlier is defined as exceeding 1.5 interval_width from the upper and lower bound.\n",
    "            margin = 1\n",
    "            if (yk > y_mean+margin*interval_width) | (yk < y_mean-margin*interval_width):\n",
    "                if self.outlier_quota == 1:\n",
    "                    # print(f'Reinitiate the particles: k={k}')\n",
    "                    xk = self.gen_x0(Ns, t[k])\n",
    "                    wk = np.repeat(1 / Ns, Ns)\n",
    "                    self.outlier_quota = self.initial_outlier_quota\n",
    "                else:\n",
    "                    # print(f'Smoothing due to outlier: k={k}')\n",
    "                    for i in range(Ns):\n",
    "                        xk[:, i] = self.sys(t[k], xkm1[:, i], np.zeros(xkm1.shape[0]-1))\n",
    "                    wk = wkm1\n",
    "                    self.outlier_quota -= 1\n",
    "            else:\n",
    "                wk = wk/sum(wk)\n",
    "                self.outlier_quota = self.initial_outlier_quota\n",
    "\n",
    "        # Resampling if necessary.\n",
    "        resample_percentaje = 0.50\n",
    "        Nt = resample_percentaje * Ns\n",
    "        # Calculate effective sample size: eq 48, Ref 1\n",
    "        Neff = 1 / sum(wk**2)\n",
    "        if Neff < Nt:\n",
    "            # print('Resampling ...')\n",
    "            xk, wk = self.resample(xk, wk, resampling_strategy)\n",
    "\n",
    "        # Compute estimated state\n",
    "        xhk = np.matmul(xk, wk)\n",
    "\n",
    "        # Store new weights and particles\n",
    "        self.w[:, k] = wk\n",
    "        self.particles[:, :, k] = xk\n",
    "\n",
    "        return xhk\n",
    "\n",
    "\n",
    "    def resample(self, xk, wk, resampling_strategy='multinomial_resampling'):\n",
    "        '''\n",
    "        This function implements the resampling of PF.\n",
    "\n",
    "        Args:\n",
    "        - xk: The original particles.\n",
    "        - wk: Weights.\n",
    "        - resampling_strategy: A string of the resamping strategy:\n",
    "            -- 'multinomial_resampling' (default): Sampling with replacement.\n",
    "            -- 'systematic_resampling': Latin hypercube sampling\n",
    "\n",
    "        Outputs:\n",
    "        - xk: The particles after resampling.\n",
    "        - wk: The new weights.\n",
    "        '''\n",
    "        Ns = len(wk)  # Ns = number of particles\n",
    "\n",
    "        if resampling_strategy == 'multinomial_resampling': # Sampling with replacement.\n",
    "            with_replacement = True\n",
    "            idx = np.random.choice(np.arange(Ns), Ns, p=wk, replace=with_replacement)\n",
    "        elif resampling_strategy == 'systematic_resampling':\n",
    "            # this is performing latin hypercube sampling on wk\n",
    "            edges = np.minimum(np.cumsum(wk), [1]*len(wk))  # protect against accumulated round-off\n",
    "            edges[-1] = 1  # get the upper edge exact\n",
    "            u1 = np.random.random()/Ns\n",
    "            # this works like the inverse of the empirical distribution and returns\n",
    "            # the interval where the sample is to be found\n",
    "            _, idx = np.histogram(np.arange(u1, 1, 1/Ns), edges)\n",
    "        else:\n",
    "            raise ValueError('Resampling strategy not implemented!')\n",
    "\n",
    "        xk = xk[:, idx]  # extract new particles\n",
    "        wk = np.ones(Ns)/Ns  # now all particles have the same weight\n",
    "\n",
    "        return xk, wk\n",
    "\n",
    "\n",
    "    def rul_prediction(self, threshold, idx_pred, t, max_RUL=145, alpha = .1):\n",
    "        '''\n",
    "        This function predicts the RUL based on the result of the PF.\n",
    "\n",
    "        Args:\n",
    "        - threshold: The failure threshold. Failure is defined when the degradation measures < threshold.\n",
    "        - t_pred: An array representing the time horizon in which you would like to make prediction. Note:\n",
    "            -- t_pred should cover t, i.e., the first part of the t_pred should contain all the measurement time instants.\n",
    "            -- The second part of t_pred contains the times corresponds to the prediction steps in particle filtering. For example, suppose a step in PF represents 10 time units and the observation ends at t=100.\n",
    "            If you wish to predict the degradation trajectory in the next 10 time step, then, t_pred = [t, 110, 120, \\cdots, 210].\n",
    "        - idx_pred: An array containing the indexes in t_pred, that represents the time instants you wish to perform RUL prediction.\n",
    "            For example, t_pred = [10, 20, 30, \\cdots, 100], and you have measurement data for the first three points $10, 20$ and $30$.\n",
    "            If you wish to predict RUL at these three time instants, then you should set idx_pred = [0, 1, 2]. By doing so, three seperate RUL predictions will be performed. Each prediction will only use the degradation\n",
    "            measurements before it.\n",
    "        - max_RUL: When the search for RUL ends without finding the failure time, we set the RUL = max_RUL. Default value is $145$.\n",
    "        - alpha: Confidence level for calculating the confidence interval. Default value is .1.\n",
    "\n",
    "        Outputs:\n",
    "        - rul_mean: An array containing the mean value of the predicted RUL. Shape (n_pred), where n_pred is the number of predictions.\n",
    "        - rul_bands: An ndarry containting the upper and lower CI for the mean RUL. Shape: (n_pred, 2)\n",
    "        - rul: An ndarray containing the predicted RULs for all the particles. Shape (Ns, n_pred) where Ns is the number of particles.\n",
    "        - rul_weights: An ndarry containing the weights for the predicted RULs. Shape (Ns, n_pred).\n",
    "        - deg_pred_mean: A list of n_pred element. Each element is the predicted degradation path at a given prediction point.\n",
    "        - deg_pred_bands: The CI of the previous variable.\n",
    "        '''\n",
    "        # Initialize the variables.\n",
    "        n_pred = len(idx_pred)\n",
    "        rul = max_RUL*np.ones((self.Ns, n_pred))\n",
    "        rul_mean = np.zeros(n_pred)\n",
    "        rul_bands = np.zeros((n_pred, 2))\n",
    "        rul_weights = np.zeros((self.Ns, n_pred))\n",
    "\n",
    "        deg_pred_mean = []\n",
    "        deg_pred_bands = []\n",
    "\n",
    "        # Do a loop to make RUL predictions:\n",
    "        for i in tqdm(range(n_pred)):\n",
    "            idx_pred_i = idx_pred[i] # Index of the prediction instant.\n",
    "            rul_weights[:, i] = self.w[:, idx_pred_i] # Get the weights.\n",
    "\n",
    "            # For each particle, use the degradation model to calculate the RUL.\n",
    "            for j in range(self.Ns):\n",
    "                x_cur = self.particles[:, j, idx_pred_i] # Get the current particles.\n",
    "                # Define a equation: Degration(t)=0, and solve for t.\n",
    "                hdl_eq = lambda tt: self.degradation_path(x_cur, tt) - threshold\n",
    "                ttf_run = fsolve(hdl_eq, t[idx_pred_i])\n",
    "                # Get RUL.\n",
    "                rul[j, i] = math.floor(ttf_run) + 1 - t[idx_pred_i]\n",
    "                if rul[j, i] > max_RUL:\n",
    "                    rul[j, i] = max_RUL\n",
    "                elif rul[j, i] < 0:\n",
    "                    rul[j, i] = 0\n",
    "            # Calculate the mean and CI for the predicted RUL.\n",
    "            rul_mean[i], rul_bands[i, :] = self.get_state_estimation(rul[:, i], rul_weights[:, i], alpha=alpha)\n",
    "\n",
    "            # Degradation state prediction:\n",
    "            n_t = math.floor(rul_mean[i]) + 10\n",
    "            deg_pred = np.zeros((self.Ns, n_t))\n",
    "            for j in range(self.Ns):\n",
    "                x_cur = self.particles[:, j, idx_pred_i] # Get the current particles.\n",
    "                tt = np.arange(t[idx_pred_i]+1, t[idx_pred_i]+1+n_t)\n",
    "                deg_pred[j, :] = self.degradation_path(x_cur, tt)\n",
    "            # Get the mean and bands of the degradation prediction.\n",
    "            deg_mean = np.zeros(n_t)\n",
    "            deg_bands = np.zeros((n_t, 2))\n",
    "            for j in range(n_t):\n",
    "                deg_mean[j], deg_bands[j, :] = self.get_state_estimation(deg_pred[:, j], rul_weights[:, i], alpha=alpha)\n",
    "            deg_pred_mean.append(deg_mean)\n",
    "            deg_pred_bands.append(deg_bands)\n",
    "\n",
    "        return rul_mean, rul_bands, rul, rul_weights, deg_pred_mean, deg_pred_bands\n",
    "\n",
    "\n",
    "    def get_state_estimation(self, x_sample, weights, alpha=.1):\n",
    "        '''\n",
    "        This function estimates the mean and CI based on particles with weights.\n",
    "\n",
    "        Args:\n",
    "            - x: An array of the particles.\n",
    "            - weights: weights.\n",
    "            - alpha: confidence level, default is .1.\n",
    "\n",
    "        Outputs:\n",
    "            - est_mean: The mean value of the estimate.\n",
    "            - est_bands: Confidence bands.\n",
    "        '''\n",
    "        # Get the mean estimation\n",
    "        est_mean = np.dot(x_sample, weights)\n",
    "\n",
    "        # Get the CI.\n",
    "        idx_sorted = np.argsort(x_sample)\n",
    "        x_sorted = x_sample[idx_sorted]\n",
    "        w_sort = weights[idx_sorted]\n",
    "        x_cdf = np.cumsum(w_sort)\n",
    "        index_l = next(i for i, x in enumerate(x_cdf) if x > alpha/2)\n",
    "        index_u = next(i for i, x in enumerate(x_cdf) if x > 1-alpha/2)\n",
    "        est_bands = np.array([x_sorted[index_l], x_sorted[index_u]])\n",
    "\n",
    "        return est_mean, est_bands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state space model.\n",
    "# Process equation x[k] = sys(k, x[k-1], u[k]):\n",
    "nx = 4  # number of states\n",
    "nu = 3  # size of the vector of process noise\n",
    "\n",
    "# Define the standard deviation of the process noise.\n",
    "sigma_u = 1*np.array([1e-2, 1e-3, 1e-2])\n",
    "\n",
    "# Define the degradation model.\n",
    "def degradation_path(x, t):\n",
    "    return x[2] / (1 + np.exp(-1*(x[0]+x[1]*t)))\n",
    "\n",
    "# Process model.\n",
    "def sys(tk, xkm1, uk):\n",
    "    xk = np.zeros(nx)\n",
    "    xk[0] = xkm1[0] + uk[0]\n",
    "    xk[1] = xkm1[1] + uk[1]\n",
    "    xk[2] = xkm1[2] + uk[2]\n",
    "    xk[3] = degradation_path(np.array([xk[0], xk[1], xk[2]]), tk)\n",
    "    return xk\n",
    "\n",
    "# Generate system noise.\n",
    "def gen_sys_noise(Ns=1):\n",
    "    if Ns == 1:\n",
    "        sample = np.random.normal(0, sigma_u)\n",
    "    else:\n",
    "        sample = np.zeros((nu, Ns))\n",
    "        for i in range(nu):\n",
    "            sample[i, :] = np.random.normal(0, sigma_u[i], size=Ns)\n",
    "    return sample\n",
    "\n",
    "# PDF of process noise and noise generator function\n",
    "def p_sys_noise(u):\n",
    "    return norm.pdf(u, 0, sigma_u)\n",
    "\n",
    "# Define observation equation.\n",
    "ny = 1  # number of observations\n",
    "nv = 1  # size of the vector of observation noise\n",
    "sigma_v = 5e-2\n",
    "\n",
    "# Observation equation y[k] = obs(k, x[k], v[k]);\n",
    "def obs(xk, vk):\n",
    "    return xk[3] + vk\n",
    "\n",
    "# PDF of observation noise and noise generator function\n",
    "def p_obs_noise(v):\n",
    "    return norm.pdf(v, 0, sigma_v)\n",
    "\n",
    "# Generate observation noise.\n",
    "def gen_obs_noise():\n",
    "    return np.random.normal(0, sigma_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_x0(Ns=1, t_0=0):\n",
    "    x0 = np.zeros((nx, Ns))\n",
    "    x0[0, :] = np.random.uniform(-6, -4, size=Ns)\n",
    "    x0[1, :] = np.random.uniform(10/60, 12/60, size=Ns)\n",
    "    x0[2, :] = np.random.uniform(.9, 1, size=Ns)\n",
    "    x0[3, :] = degradation_path(np.array([x0[0], x0[1], x0[2]]), t_0)\n",
    "    return x0\n",
    "\n",
    "# Observation likelihood.\n",
    "def p_yk_given_xk(yk, xk):\n",
    "    return p_obs_noise(yk - obs(xk, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering items based on the number of rows\n",
    "unique_item_ids = all_test_data['item_id'].unique()\n",
    "items_to_estimate = []\n",
    "\n",
    "for item_id in unique_item_ids:\n",
    "    item_data = all_test_data[all_test_data['item_id'] == item_id]\n",
    "    if len(item_data) > 5:\n",
    "        items_to_estimate.append(item_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/tmp/ipykernel_72385/2808879871.py:234: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  rul[j, i] = math.floor(ttf_run) + 1 - t[idx_pred_i]\n",
      "/home/user/anaconda3/lib/python3.11/site-packages/scipy/optimize/_minpack_py.py:177: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Constants and threshold setup\n",
    "threshold = 0.85\n",
    "max_RUL = 60  # Assuming you want to limit the RUL predictions to a maximum value\n",
    "working_after_6_months = {}\n",
    "\n",
    "for item_id in items_to_estimate:\n",
    "    item_data = all_test_data[all_test_data['item_id'] == item_id]\n",
    "    t = item_data['time (months)'].to_numpy()\n",
    "    y = item_data['crack length (arbitary unit)'].to_numpy().reshape(1, -1)\n",
    "    T = len(t)  # Number of time steps\n",
    "    t_0 = t[0] if T > 0 else 0  # Use the first time step or 0 if there are no steps\n",
    "\n",
    "    # This adjustment is correct if t_0 is fixed and known at the time of pf_class initialization\n",
    "    pf = pf_class(\n",
    "        Ns=1000, \n",
    "        t=t, \n",
    "        nx=nx, \n",
    "        gen_x0=lambda Ns, t_0=t[0]: gen_x0(Ns, t_0),  # Adjusted lambda to include t_0 with a default value\n",
    "        sys=sys, \n",
    "        obs=obs,\n",
    "        p_yk_given_xk=p_yk_given_xk, \n",
    "        gen_sys_noise=gen_sys_noise,\n",
    "        degradation_path=degradation_path\n",
    "    )\n",
    "\n",
    "    # Initialize particle filter and perform state estimation\n",
    "    for k in range(1, T):\n",
    "        pf.k = k\n",
    "        pf.state_estimation(y[0, k])\n",
    "    \n",
    "    # Predict RUL\n",
    "    rul_mean, _, _, _, _, _ = pf.rul_prediction(threshold, [T-1], t, max_RUL=max_RUL)\n",
    "    \n",
    "    # Check if the item is expected to work beyond 6 months\n",
    "    is_working_after_6_months = rul_mean > 6\n",
    "    working_after_6_months[item_id] = is_working_after_6_months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([ True]),\n",
       " 1: array([ True]),\n",
       " 2: array([ True]),\n",
       " 5: array([ True]),\n",
       " 6: array([ True]),\n",
       " 9: array([False]),\n",
       " 10: array([ True]),\n",
       " 11: array([ True]),\n",
       " 13: array([ True]),\n",
       " 14: array([ True]),\n",
       " 15: array([ True]),\n",
       " 17: array([ True]),\n",
       " 18: array([ True]),\n",
       " 20: array([ True]),\n",
       " 21: array([ True]),\n",
       " 23: array([ True]),\n",
       " 24: array([ True]),\n",
       " 25: array([ True]),\n",
       " 26: array([ True]),\n",
       " 28: array([ True]),\n",
       " 29: array([ True]),\n",
       " 30: array([ True]),\n",
       " 32: array([ True]),\n",
       " 33: array([ True]),\n",
       " 36: array([ True]),\n",
       " 37: array([ True]),\n",
       " 38: array([ True]),\n",
       " 39: array([ True]),\n",
       " 40: array([ True]),\n",
       " 42: array([ True]),\n",
       " 44: array([ True]),\n",
       " 46: array([ True]),\n",
       " 47: array([ True]),\n",
       " 49: array([ True])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_after_6_months"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
